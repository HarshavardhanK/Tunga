{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Jupyter Notebook for the CS205 Final Project\n",
    "\n",
    "In this project we will explore a usecase of Large Language Models\n",
    " \n",
    "We will start with how to use an LLM through HuggingFace, and explain some of the basic concepts behind an LLM. Once we have a good understading of how to use an LLM for generating text, we will explore Retrieval Augmented Generation (RAG). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project we have used Llama 2 7 Billion paramter model with OpenAI's text-embed-002 embedding model. Llama 2 7B was served locally by Ollama. We have used Llama Index and LangChain to interact with the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data store is at the root of the project directory with the name 'data'. Create a data repository before running the indexing and query cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's understand how to use an LLM using HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#Import HuggingFace Transformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch Meta's OPT LLM with 1.3 billion parameters. This is quite a small model compared to the SOTA like GPT4V, etc.\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('facebook/opt-1.3b')\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/opt-1.3b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Open Pre-trained Transformer (OPT) is a collection of decoder-only transformer developed by Meta. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = 'I like CS205 Artificial Intelligence course, because'\n",
    "tok_input = tokenizer(input_text, return_tensors='pt', add_special_tokens=True, truncation=True) #Create tokens from the given input and return PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "generated_output = model.generate(**tok_input, \n",
    "                                  max_new_tokens=200, \n",
    "                                  return_dict_in_generate=True, \n",
    "                                  do_sample=True) #Generation is deterministic. \n",
    "                                                  #To use top-k sampling, set do_sample=True to get different responses in each generation\n",
    "                                                  #Set do_sample=False to have a deterministic generation each time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_output = tokenizer.batch_decode(generated_output.sequences, skip_special_tokens=True)[0]\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We saw text generation in the previous subsection. Now lets explore text summarization, a critical usecase of LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is an embedding?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is RAG?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Retrieval Augmented Generation is a technique in generative AI to boost the knowledge of an LLM. The LLM parameters are learned and not updated to the current information, so a specialized database of knowledge (could be private) is created for the LLM is access. This is a non-parametric memory, i.e, this information is not stored in the learned paramaters of the LLM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing RAG with Llama Index using ChromaDB as the vector database. Ollama is used to serve Llama 2 locally\n",
    "\n",
    "The data can be accessed at this link. Add this folder to the root of the project directory\n",
    "\n",
    "https://drive.google.com/drive/folders/10wzRErO4Zlj6L3bLSqh9QtTLDkaUOuxS?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext, download_loader\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from langchain.llms import Ollama\n",
    "from dotenv import dotenv_values\n",
    "from pathlib import Path\n",
    "import chromadb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = dotenv_values('../.env')[\"OPENAI_API_KEY\"]\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the embedding\n",
    "\n",
    "llm = Ollama(model=\"llama2\")\n",
    "#embed_model = OllamaEmbeddings(base_url=\"http://localhost:11434\", model=\"llama2\") #Local Llama 2 embedding model\n",
    "embed_model = OpenAIEmbedding() #Using OpenAI's text-embed-002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION = \"aiprof\"\n",
    "SLIDE_COLLECTION = 'slides'\n",
    "PATH = '../chroma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create client and a new collection\n",
    "db = chromadb.PersistentClient(path=PATH)\n",
    "chroma_collection = db.get_or_create_collection(COLLECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "service_context = ServiceContext.from_defaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"../data/AIMA\", recursive=True).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "slides_reader = download_loader(\"PptxReader\")\n",
    "loader = slides_reader()\n",
    "slides = []\n",
    "\n",
    "for file in os.listdir(\"../data/slides/\"):\n",
    "    slides += loader.load_data(\"../data/slides/\" + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "information = documents + slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    information, storage_context=storage_context, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk\n",
    "db2 = chromadb.PersistentClient(path=PATH)\n",
    "chroma_collection = db2.get_or_create_collection(COLLECTION)\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "index2 = VectorStoreIndex.from_vector_store(\n",
    "    vector_store,\n",
    "    service_context=service_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index2.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rational agent approach refers to designing an intelligent agent that makes decisions and takes actions based on rationality. A rational agent is one that takes actions that maximize its expected utility, given its knowledge and beliefs about the world. This approach involves defining the agent's goals, specifying its knowledge and capabilities, and designing algorithms or mechanisms for decision-making and action selection. The rational agent approach is a fundamental concept in artificial intelligence and is used to develop intelligent systems that can solve complex problems and make optimal decisions.\n"
     ]
    }
   ],
   "source": [
    "resp = query_engine.query(\"What is The rational agent approach?\")\n",
    "print(resp.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = query_engine.query(\"Generate 2 concise questions about rational agents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What are some key characteristics of rational agents?',\n",
       " 'How do rational agents make decisions?']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.response.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function HILL-CLIMBING(problem) returns a solution state\n",
      "     inputs: problem, a problem\n",
      "     static: current, a node; next, a node\n",
      "     current <- MAKE-NODE(INITIAL-STATE[problem])\n",
      "     loop do\n",
      "         next <- a highest-valued successor of current\n",
      "         if VALUE[next] < VALUE[current] then return current\n",
      "         current <- next\n",
      "     end\n"
     ]
    }
   ],
   "source": [
    "resp = query_engine.query(\"write pseudo code for Hill Climbing Local Search\")\n",
    "print(resp.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expectations for the Fall 2023 final project include preparing a tutorial lesson using Google CoLab or a similar platform to demonstrate a favorite Machine Learning technique or an advanced topic in AI. The project should include a motivating question, a real dataset related to the question, and a method to approach answering the question. The project should be unique and have the student's stamp of uniqueness. It should be structured like an end-to-end tutorial with clear illustrations and explanations. All code should be well-commented and understandable. Preparatory work should be clearly documented. The tutorial should be succinct, complete, and submitted via a Google Form. The deadline for the project is tentatively set for November 30, 2023.\n"
     ]
    }
   ],
   "source": [
    "resp = query_engine.query(\"what are the expectations for fall 2023 final project and when is the deadline\")\n",
    "print(resp.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the information about why Pluto is not considered a planet is not present in the given context.\n"
     ]
    }
   ],
   "source": [
    "resp = query_engine.query(\"why is pluto not a planet? is this information present in the database\")\n",
    "print(resp.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no information in the given context that indicates whether the content is censored or not.\n"
     ]
    }
   ],
   "source": [
    "resp = query_engine.query(\"is this censored?\")\n",
    "print(resp.response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs205",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
